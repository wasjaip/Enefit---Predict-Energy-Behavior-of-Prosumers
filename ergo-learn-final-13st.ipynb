{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7346471,"sourceType":"datasetVersion","datasetId":4265932},{"sourceId":7363708,"sourceType":"datasetVersion","datasetId":4126877},{"sourceId":7462682,"sourceType":"datasetVersion","datasetId":4343530}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Импорт библиотек","metadata":{}},{"cell_type":"code","source":"import time\n# Время старта работы ноутбука\nnotebook_starttime = time.time()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nimport gc\nimport pickle\nfrom itertools import zip_longest\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nfrom joblib import dump\nfrom joblib import load\n\nimport catboost as cb\n\nimport optuna\n\nimport matplotlib.pyplot as plt\n\n#from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","editable":true,"slideshow":{"slide_type":""},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Настройка: сабмит или локально","metadata":{}},{"cell_type":"code","source":"# Ставим is_local в True, если локально работаем, если сабмитим - ставим в False\nis_local = False\n#is_local = True\n\n# Ставим is_gpu в True, если будем работать на GPU, если на процессоре - ставим в False\n# is_gpu = False\nis_gpu = True\n\n# Ставим is_tuning в True, если запускаем подбор гиперпараметров в Optuna\nis_tuning = False\n\n# Начальная дата обучения модели\ntraining_start_date = 'datetime >= \"2022-01-01 00:00:00\"'\n\n\n# Устанавливаем время начало переобучения. Начинаем переобучать модели заново,\n# когда подходит дата предсказаний, которая идет в скор.\n# scor_start_time = pd.to_datetime('2023-06-01')\n\n# Устанавливаем время завершения переобучения и предсказаний.\n# после этой даты просто возвращаем те предсказания что дали.\n# scor_stop_time = pd.to_datetime('2023-09-01')\n\n# Максимальное время работы ноутбука, когда еще можно тренировать модели\nmax_train_duration = (8*60*60 + 60*50)\n\n# Время после которого не осуществляем тренировку моделей\nstop_train_time = notebook_starttime + max_train_duration\n\n# Ставим в False, если не хотитим отключать контроль времени выполнения ноутбука\n# (не обучать модели заново после 8 часов 30 минут)\n# Если хотим, чтобы модели обучались заново и после лимита, ставим True\n#is_disable_run_time_limit = True\nis_disable_run_time_limit = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Настройки графического процессора\nif is_local:\n    # Для домашней машины\n    import lightgbm as lgb\n    # Число процессов параллельного обучения модетей и предсказания\n    #num_processes = 6\n    num_processes = 1\n    # число параллельных threads для тренировки каждой модели\n    n_jobs = None\n    #n_jobs = 1\n    # Тип ускорителя в системе: gpu или cuda\n    gpu_type = 'gpu'\n    # число GPU в системе\n    gpus_n = 1\nelse:\n    # Для карточки на кагле T4x2\n    !pip uninstall -y lightgbm\n    !pip install /kaggle/input/lightgbm420-cuda/lightgbm-4.2.0-py3-none-manylinux_2_35_x86_64.whl\n    import lightgbm as lgb\n    \n    num_processes = 2\n    # число параллельных threads для тренировки каждой модели\n    n_jobs = None\n    # Тип ускорителя в системе: gpu или cuda\n    gpu_type = 'cuda'\n    # число GPU в системе\n    gpus_n = 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Версия lightgbm:\", lgb.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Возвращает сколько уже работает ноутбук\ndef p_time():\n    run_time = round(time.time() - notebook_starttime)\n    return str(run_time).zfill(5)+' sec:'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MonthlyKFold:\n    def __init__(self, n_splits=3):\n        self.n_splits = n_splits\n        \n    def split(self, X, y, groups=None):\n        dates = 12 * X[\"year\"] + X[\"month\"]\n        timesteps = sorted(dates.unique().tolist())\n        X = X.reset_index()\n        \n        for t in timesteps[-self.n_splits:]:\n            idx_train = X[dates.values < t].index\n            idx_test = X[dates.values == t].index\n            \n            yield idx_train, idx_test\n            \n    def get_n_splits(self, X, y, groups=None):\n        return self.n_splits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_tuning:\n    cv = MonthlyKFold()\n    CV = GroupTimeSeriesSplit(test_size=3, n_splits=3, window_type='rolling', shift_size=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering sub","metadata":{}},{"cell_type":"code","source":"def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days):\n    print(p_time(), 'feature_eng: Start')\n    working_days = (\n        working_days\n        .with_columns(\n            pl.col(\"date\").cast(pl.Date)\n        )\n    )\n    \n    df_data = (\n        df_data\n        .with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n    )\n    \n    df_client = (\n        df_client\n        .with_columns(\n            (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n        )\n    )\n    \n    df_gas = (\n        df_gas\n        .rename({\"forecast_date\": \"date\"})\n        .with_columns(\n            (pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)\n        )\n    )\n    \n    df_electricity = (\n        df_electricity\n        .rename({\"forecast_date\": \"datetime\"})\n        .with_columns(\n            pl.col(\"datetime\") + pl.duration(days=1)\n        )\n    )\n    \n    df_location = (\n        df_location\n        .with_columns(\n            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n            pl.col(\"longitude\").cast(pl.datatypes.Float32)\n        )\n    )\n    \n    df_forecast = (\n        df_forecast\n        .rename({\"forecast_datetime\": \"datetime\"})\n        .with_columns(\n            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            #pl.col('datetime').dt.convert_time_zone(\"Europe/Bucharest\").dt.replace_time_zone(None).cast(pl.Datetime(\"us\")),\n            pl.col('datetime').dt.replace_time_zone(None).cast(pl.Datetime(\"us\"))\n            #pl.col('datetime').cast(pl.Datetime)\n        )\n        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n        .drop(\"longitude\", \"latitude\")\n    )\n    \n    df_historical = (\n        df_historical\n        .with_columns(\n            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            pl.col(\"datetime\") + pl.duration(hours=37)\n        )\n        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n        .drop(\"longitude\", \"latitude\")\n        #.with_columns(\n        #    (pl.col(\"direct_solar_radiation\")+pl.col(\"diffuse_radiation\")).alias(\"rad_sum\"),\n        #)\n    )\n    \n    df_forecast_date = (\n        df_forecast\n        .group_by(\"datetime\").mean()\n        .drop(\"county\")\n    )\n    \n    df_forecast_local = (\n        df_forecast\n        .filter(pl.col(\"county\").is_not_null())\n        .group_by(\"county\", \"datetime\").mean()\n    )\n    \n    df_historical_date = (\n        df_historical\n        .group_by(\"datetime\").mean()\n        .drop(\"county\")\n    )\n    \n    df_historical_local = (\n        df_historical\n        .filter(pl.col(\"county\").is_not_null())\n        .group_by(\"county\", \"datetime\").mean()\n    )\n    # Объединение всех обработанных данных с основным датафреймом df_data\n    df_data = (\n        df_data\n        .join(df_gas, on=\"date\", how=\"left\")\n        .join(df_client, on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n        .join(df_electricity, on=\"datetime\", how=\"left\")\n        \n        .join(df_forecast_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n        .join(df_forecast_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n        .join(df_historical_date, on=\"datetime\", how=\"left\", suffix=\"_hd\")\n        .join(df_historical_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl\")\n        \n\n        .join(df_forecast_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fd7\")\n        .join(df_forecast_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl7\")\n        .join(df_historical_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hd7\")\n        .join(df_historical_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl7\")\n\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        #.join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=8)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=9)).rename({\"target\": \"target_8\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=10)).rename({\"target\": \"target_9\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=11)).rename({\"target\": \"target_10\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=12)).rename({\"target\": \"target_11\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=13)).rename({\"target\": \"target_12\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_13\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=353)).rename({\"target\": \"target_y_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=361)).rename({\"target\": \"target_y_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=362)).rename({\"target\": \"target_y_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=363)).rename({\"target\": \"target_y_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=364)).rename({\"target\": \"target_y_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=365)).rename({\"target\": \"target_y_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=366)).rename({\"target\": \"target_y_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=367)).rename({\"target\": \"target_y_8\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        \n        #Добавляем коолонку рабочий день или нет.\n        .join(working_days, on=\"date\", how=\"left\")\n        # Создание категориальных признаков и тригонометрических функций времени\n        .with_columns(\n            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"), # Добавление номера дня в году\n            pl.col(\"datetime\").dt.hour().alias(\"hour\"),# Добавление часа\n            pl.col(\"datetime\").dt.day().alias(\"day\"),# Добавление дня\n            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),# Добавление дня недели\n            pl.col(\"datetime\").dt.month().alias(\"month\"),# Добавление месяца\n            pl.col(\"datetime\").dt.year().alias(\"year\"),# Добавление года\n        )\n        # Приведение типов данных\n        .with_columns(\n            pl.concat_str(\"county\", \"is_business\", \"product_type\", \"is_consumption\", separator=\"_\").alias(\"category_1\"),\n        )\n        \n        .with_columns(\n            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"), # Тригонометрические функции для дня в году\n            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n        )\n        \n        .with_columns(\n            pl.col(pl.Float64).cast(pl.Float32),\n        )\n\n        # Новая фича. Назвал cap_rad_temp. нашел в обсуждениях: \n        # https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/468654\n        .with_columns(\n            (pl.col(\"installed_capacity\")*pl.col(\"surface_solar_radiation_downwards\")/(pl.col('temperature') + 273.15)).alias(\"cap_rad_temp\"),\n        )\n\n    )\n\n    for hours_lag in [1, 2]:\n            df_data = df_data.join(\n                df_forecast_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_data = df_data.join(\n                df_forecast_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n    df_data = (\n        # Удаление ненужных колонок\n        df_data.drop(\"date\", \"datetime\", \"hour\", \"dayofyear\")\n    )\n    '''\n    # Делаем колонку со сдвигом на один день для cap_rad_temp\n    df_cap_rad_temp = df_data[['cap_rad_temp',\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"]]\n    df_data = (\n        df_data\n        .join(df_cap_rad_temp.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"cap_rad_temp\": \"cap_rad_temp_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n        # Удаление ненужных колонок\n        .drop(\"date\", \"datetime\", \"hour\", \"dayofyear\")\n    )\n    '''\n    print(p_time(), 'feature_eng: End')\n    # return df_data, df_historical_local\n    return df_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_pandas(X, y=None):\n    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n    print(p_time(), 'to_pandas: Start')\n    if y is not None:\n        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n    else:\n        df = X.to_pandas()    \n\n    print(p_time(), 'to_pandas: Start make Features')\n\n    \n    df = df.set_index(\"row_id\")\n    df[cat_cols] = df[cat_cols].astype(\"category\")\n    \n\n    #df[\"target_skew_1\"] = df[[f\"target_{i}\" for i in range(1, 14)]].skew(1)\n\n    df[\"target_ratio_1\"] = df[\"target_1\"] / (df[\"target_13\"] + 1e-3)\n    df[\"target_mean_1\"] = df[[f\"target_{i}\" for i in range(1, 14)]].mean(1)\n    df[\"target_std_1\"] = df[[f\"target_{i}\" for i in range(1, 14)]].std(1)\n\n    df[\"target_ratio_2\"] = df[\"target_1\"] / (df[\"target_7\"] + 1e-3)\n    df[\"target_mean_2\"] = df[[f\"target_{i}\" for i in range(1, 8)]].mean(1)\n    df[\"target_std_2\"] = df[[f\"target_{i}\" for i in range(1, 8)]].std(1)\n\n    df[\"target_ratio_y_1\"] = df[\"target_y_1\"] / (df[\"target_y_8\"] + 1e-3)\n    df[\"target_mean_y_1\"] = df[[f\"target_y_{i}\" for i in range(1, 9)]].mean(1)\n    df[\"target_std_y_1\"] = df[[f\"target_y_{i}\" for i in range(1, 9)]].std(1)\n    #df[\"target_ratio_y_2\"] = df[\"target_y_2\"] / (df[\"target_y_3\"] + 1e-3)\n    print(p_time(), 'to_pandas: End')\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_list = ['target_{}'.format(i) for i in range(1, 13+1)]\n\nprint(target_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n# Для локальных вычислений. Последний data_block_id тренировочной выборки\n# А начиная со следующего data_block_id и до конца идет тест\n# train_end_data_block_id = 517\n# train_end_data_block_id = 600+360\n# train_end_data_block_id = 600\ntrain_end_data_block_id = 598\n#train_end_data_block_id = 636\n# train_end_data_block_id = 680\n\ndata_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id', 'data_block_id']\n# В df_data_cols колонки в таком порядке в каком они потом формируются в df_data\ndf_data_cols     = ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'datetime', 'data_block_id', 'row_id']\nclient_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\ngas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\nelectricity_cols = ['forecast_date', 'euros_per_mwh']\nforecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\nhistorical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\nlocation_cols    = ['longitude', 'latitude', 'county']\ntarget_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n\nsave_path = None\nload_path = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# для оптуны\n# Диапазон гиперпараметров для модели обучающейся на данных is_consumption=1 \ndef lgb_objective_cons1(trial):\n    params = {\n        'device'            : trial.suggest_categorical('device', ['gpu']),\n        'gpu_platform_id'   : trial.suggest_int('gpu_platform_id', 1, 1),\n        'gpu_device_id'     : trial.suggest_int('gpu_device_id', 0, 0),\n        'n_estimators'      : trial.suggest_int('n_estimators', 1000, 2000),\n        'verbose'           : trial.suggest_int('verbose', -1, -1),\n        'random_state'      : trial.suggest_int('random_state', 42, 42),\n        'objective'         : trial.suggest_categorical('objective', ['l2']),\n        'num_leaves'        : trial.suggest_int('num_leaves', 20, 50),\n        'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 0.1),\n        'colsample_bytree'  : trial.suggest_float('colsample_bytree', 0.1, 1.0),\n        'colsample_bynode'  : trial.suggest_float('colsample_bynode', 0.1, 1.0),\n        #'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 20.0),\n        'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 10.0),\n        'reg_lambda'        : trial.suggest_float('reg_lambda', 1e-2, 20.0),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 4, 256),\n        'max_depth'         : trial.suggest_int('max_depth', -1, -1),\n        'max_bin'           : trial.suggest_int('max_bin', 32, 200),\n    }\n    \n    model  = lgb.LGBMRegressor(**params)\n    X      = df_train[df_train['is_consumption']==1].drop(columns=[\"target\", \"datetime\"]).reset_index(drop=True)\n    y      = df_train[df_train['is_consumption']==1][\"target\"].reset_index(drop=True)\n    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n    \n    return -np.mean(scores)\n\n# Диапазон гиперпараметров для модели обучающейся на данных is_consumption=0\ndef lgb_objective_cons0(trial):\n    params = {\n        'device'            : trial.suggest_categorical('device', ['gpu']),\n        'gpu_platform_id'   : trial.suggest_int('gpu_platform_id', 1, 1),\n        'gpu_device_id'     : trial.suggest_int('gpu_device_id', 0, 0),\n        'n_estimators'      : trial.suggest_int('n_estimators', 1000, 2000),\n        'verbose'           : trial.suggest_int('verbose', -1, -1),\n        'random_state'      : trial.suggest_int('random_state', 42, 42),\n        'objective'         : trial.suggest_categorical('objective', ['l2']),\n        'num_leaves'        : trial.suggest_int('num_leaves', 20, 50),\n        'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 0.1),\n        'colsample_bytree'  : trial.suggest_float('colsample_bytree', 0.1, 1.0),\n        'colsample_bynode'  : trial.suggest_float('colsample_bynode', 0.1, 1.0),\n        #'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 20.0),\n        #'reg_lambda'        : trial.suggest_float('reg_lambda', 1e-2, 20.0),\n        'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 10.0),\n        'reg_lambda'        : trial.suggest_float('reg_lambda', 12, 20.0),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 4, 256),\n        'max_depth'         : trial.suggest_int('max_depth', -1, -1),\n        'max_bin'           : trial.suggest_int('max_bin', 32, 200),\n    }\n    \n    model  = lgb.LGBMRegressor(**params)\n    X      = df_train[df_train['is_consumption']==0].drop(columns=[\"target\", \"datetime\"]).reset_index(drop=True)\n    y      = df_train[df_train['is_consumption']==0][\"target\"].reset_index(drop=True)\n    scores = cross_val_score(model, X, y, groups=groups, cv=cv, scoring='neg_mean_absolute_error')\n    \n    return -np.mean(scores)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global Variables","metadata":{}},{"cell_type":"markdown","source":"## Исследование","metadata":{}},{"cell_type":"code","source":"if is_local:\n    # Загрузка данных об энергопотреблении\n    train = pd.read_csv(os.path.join(root, \"train.csv\"))\n    \n    # Создание сводной таблицы с средними значениями целевой переменной (target)\n    # для каждой комбинации даты, округа, типа продукта, бизнеса и потребления\n    pivot_train = train.pivot_table(\n        index='datetime',\n        columns=['county', 'product_type', 'is_business', 'is_consumption'],\n        values='target',\n        aggfunc='mean'\n    )\n    \n    # Переименование колонок для удобства доступа и интерпретации\n    pivot_train.columns = ['county{}_productType{}_isBusiness{}_isConsumption{}'.format(*col) for col in pivot_train.columns.values]\n    pivot_train.index = pd.to_datetime(pivot_train.index)\n    \n    pivot_train","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2023 год ","metadata":{}},{"cell_type":"code","source":"if is_local:\n    # Копирование сводной таблицы для визуализации\n    df_plot = pivot_train.copy()\n    \n    # Нормализация данных для визуализации\n    df_plot = (df_plot - df_plot.min()) / (df_plot.max() - df_plot.min())\n    \n    # Ресемплирование данных по дням и вычисление средних значений\n    df_plot_resampled_D = df_plot.resample('D').mean()\n    \n    # Визуализация нормализованных данных с прозрачностью (alpha=0.1)\n    df_plot_resampled_D.loc['2022-7':].plot(alpha=0.1, color='green', figsize=(18, 6), legend=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    # Выбор колонок, соответствующих различным категориям потребления\n    columns_consumption_0 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption0')]\n    columns_consumption_1 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption1')]\n    \n    # Создание фигуры для визуализации\n    plt.figure(figsize=(15, 6))\n    \n    # Создание пустых линий для легенды\n    plt.plot([], color='red', label='is_Consumption = 1')  # Изменено на желтый цвет\n    plt.plot([], color='black', label='is_Consumption = 0')   # Изменено на черный цвет\n    \n    # Отображение легенды\n    plt.legend()\n    \n    # Визуализация данных для 'is_Consumption = 0' черным цветом\n    for column in columns_consumption_0:\n        df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='black', legend=False)  # Изменено на черный\n    \n    # Визуализация данных для 'is_Consumption = 1' желтым цветом\n    for column in columns_consumption_1:\n        df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='red', legend=False)  # Изменено на желтый\n    \n    # Отображение графика\n    plt.show()\n\n\n","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Подготовка данных","metadata":{}},{"cell_type":"markdown","source":"### Запись тестовых и тренировочных csv файлов","metadata":{}},{"cell_type":"code","source":"if is_local:\n    # Если выполняем локально\n    # Создаем пути для локальных данных с трейном и тестовой выборкой\n    train_path = 'train'\n    os.makedirs(train_path, exist_ok=True)\n    test_path = 'example_test_files'\n    os.makedirs(test_path, exist_ok=True)\n    \n    # Создаем пути для увеличенных данных по май 2024 года\n    full_train_path = 'full_train'\n    os.makedirs(full_train_path, exist_ok=True)\n    full_test_path = 'full_example_test_files'\n    os.makedirs(full_test_path, exist_ok=True)\n    full_root_path = 'full_predict-energy-behavior-of-prosumers'\n    os.makedirs(full_root_path, exist_ok=True)\nelse:\n    # Если сабмит\n    train_path = root\n# Путь, куда запишем csv файлы для теста","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Увеличиваем исходные данные по май 2024 года","metadata":{}},{"cell_type":"code","source":"# Увеличивает датафрейм по май 2024\n# df - датафрейм который увеличиваем\n# date_col колонка по которой вырезаем старые данные\n# колонки с данными к которым дабавляем 365 дней\ndef enlarge_df(df_name, date_col, add_date_cols):\n\n    print('Увеличиваем размер для:', df_name)\n    df = pd.read_csv(os.path.join(root, df_name))\n    for col in add_date_cols:\n        df[col] = pd.to_datetime(df[col])\n    \n    start_date = df[date_col].max() + pd.DateOffset(days=1)\n    # Конечная дата по которую увеличить датафрейм    \n    end_date = pd.to_datetime('2024-05-01')\n    \n    old_start_date = start_date - pd.DateOffset(days=365)\n    old_end_date = end_date - pd.DateOffset(days=365)\n    # Создаем булев индекс для среза датафрейма\n    mask = (df[date_col] >= old_start_date) & (df[date_col] <= old_end_date)\n    \n    # Вырезаем добавочный кусок из данных\n    # Применяем булев индекс для получения среза\n    result_df = df[mask].copy()\n\n    # Добавляем к data_block_id\n    if 'data_block_id' in df.columns:\n        result_df['data_block_id'] = result_df['data_block_id'] + 365\n\n    # Добавляем к датам\n    for col in add_date_cols:\n        result_df[col] = result_df[col] + pd.DateOffset(days=365)\n    \n    df = pd.concat([df, result_df], ignore_index=True)\n    df.to_csv(os.path.join(full_root_path, df_name), index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Сборка разделения файлов\ndef make_full_data():\n    enlarge_df('train.csv', 'datetime', ['datetime'])\n    enlarge_df('client.csv', 'date', ['date'])\n    enlarge_df('gas_prices.csv', 'origin_date', ['origin_date','forecast_date'])\n    enlarge_df('electricity_prices.csv', 'origin_date', ['origin_date','forecast_date'])\n    enlarge_df('forecast_weather.csv', 'origin_datetime', ['origin_datetime','forecast_datetime'])\n    enlarge_df('historical_weather.csv', 'datetime', ['datetime'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ставим в True, если нужно провести большой тест на 9 месяцев\n# и увеличение данных до мая 2024 года\n\n# is_full_test = True\nis_full_test = False\n\nif is_full_test and is_local:\n    make_full_data()\n    root = full_root_path\n    train_path = full_train_path\n    test_path = full_test_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Разделяет датафрейм на тренировочную и тестовую часть\n# Возвращает часть датафрейма для тренировки, тестовую часть датафрейма записывает в каталог с тестами\ndef split_train_test(filename):\n    df = pd.read_csv(os.path.join(root, filename))\n    \n    #Запишем часть данных для теста\n    test_df = df[df[\"data_block_id\"] > train_end_data_block_id]\n    if (filename ==\"train.csv\"):\n        # Берем только те ячейки где target был не нулевым\n        test_df = test_df[test_df[\"target\"].notnull()]\n        \n    test_df.to_csv(os.path.join(test_path, filename), index=False)\n\n    #Запишем часть данных для трейна\n    train_df = df[df[\"data_block_id\"] <= train_end_data_block_id]\n    train_df.to_csv(os.path.join(train_path, filename), index=False)\n\n# Доводим до ума тестовые таблицы чтобы они были точно такие как в реальном сабмите\ndef test_dfs_tune():\n    # Делаем таблицу revealed_targets.csv\n    df = pd.read_csv(os.path.join(root, \"train.csv\"))\n    df = df[df[\"data_block_id\"] > train_end_data_block_id - 2]\n    df[\"data_block_id\"] += 2\n    df = df[df[\"target\"].notnull()]\n    df.to_csv(os.path.join(test_path, 'revealed_targets.csv'), index=False)\n    \n    # Делаем таблицу test.csv\n    df = pd.read_csv(os.path.join(test_path, \"train.csv\"))\n    df.rename(columns={'datetime': 'prediction_datetime'}, inplace=True)\n    df.drop('target', axis=1, inplace=True)\n\n    # По умаолчанию задаем что оцениваться будет весь тестовый датасет\n    df['currently_scored'] = True\n    \n    '''\n    # Можно расскомментировать этот блок для проверки локально что работает выделение только части data_block_id,\n    # которые будут оцениваться и на которых будем делать предсказание\n    df['currently_scored'] = False\n    #df.loc[(df['data_block_id'] >= 518) & (df['data_block_id'] <= 525), 'currently_scored'] = True\n    df.loc[(df['data_block_id'] >= 518+365) & (df['data_block_id'] <= 606+365), 'currently_scored'] = True\n    #df.loc[(df['data_block_id'] >= 690) & (df['data_block_id'] <= 606+365), 'currently_scored'] = True\n    '''\n    df.to_csv(os.path.join(test_path, 'test.csv'), index=False)\n    \n    # Делаем таблицу sample_submission.csv\n    selected_columns = ['row_id', 'data_block_id']\n    df = df[selected_columns]\n    df['target'] = 0\n    df.to_csv(os.path.join(test_path, 'sample_submission.csv'), index=False)\n\n# Сборка разделения файлов\ndef make_split():\n    # csv файлы которые будем делить:\n    csv_names = [\"train.csv\", \"client.csv\", \"gas_prices.csv\", \"electricity_prices.csv\", \"forecast_weather.csv\", \"historical_weather.csv\"]\n    for csv_name in csv_names:\n        split_train_test(csv_name)\n    # Доделываем тестовые таблицы\n    test_dfs_tune()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Создаем файлы csv c тренировочными и тестовыми таблицами\nif is_local:\n    # Пока отключил создание тестовых файлом. У меня локально они есть\n    # make_split()\n    pass","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data I/O","metadata":{}},{"cell_type":"code","source":"%%time\ndf_data        = pl.read_csv(os.path.join(train_path, \"train.csv\"), columns=data_cols, try_parse_dates=True)\ndf_client      = pl.read_csv(os.path.join(train_path, \"client.csv\"), columns=client_cols, try_parse_dates=True)\ndf_gas         = pl.read_csv(os.path.join(train_path, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\ndf_electricity = pl.read_csv(os.path.join(train_path, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\ndf_forecast    = pl.read_csv(os.path.join(train_path, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\ndf_historical  = pl.read_csv(os.path.join(train_path, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n#df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\ndf_location    = pl.read_csv('/kaggle/input/locations/county_lon_lats.csv', columns=location_cols, try_parse_dates=True)\ndf_target      = df_data.select(target_cols)\nworking_days   = pl.read_csv('/kaggle/input/working-days/working_days.csv', try_parse_dates=True)\n\nschema_data        = df_data.schema\nschema_client      = df_client.schema\nschema_gas         = df_gas.schema\nschema_electricity = df_electricity.schema\nschema_forecast    = df_forecast.schema\nschema_historical  = df_historical.schema\nschema_target      = df_target.schema","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HyperParam Optimization","metadata":{}},{"cell_type":"code","source":"# Подготовка данных для поиска гиперпараметров\nif is_tuning:\n    X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n    X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n    df_train = to_pandas(X, y)\n\n    # df_train = df_train[df_train[\"target\"].notnull()].query(training_start_date)\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Подбор гиперпараметров для модели is_consumption=1\nif is_tuning:\n    study = optuna.create_study(direction='minimize', study_name='Regressor')\n    study.optimize(lgb_objective_cons1, n_trials=100, show_progress_bar=True)\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Подбор гиперпараметров для модели is_consumption=0\nif is_tuning:\n    study = optuna.create_study(direction='minimize', study_name='Regressor')\n    study.optimize(lgb_objective_cons0, n_trials=100, show_progress_bar=True)\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation","metadata":{}},{"cell_type":"code","source":"'''result = cross_validate(\n    estimator=lgb.LGBMRegressor(**best_params, random_state=42),\n    X=df_train.drop(columns=[\"target\"]), \n    y=df_train[\"target\"],\n    scoring=\"neg_mean_absolute_error\",\n    cv=MonthlyKFold(1),\n)\n\nprint(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\nprint(f\"Score Time(s): {result['score_time'].mean():.3f}\")\nprint(f\"Error(MAE): {-result['test_score'].mean():.3f}\")'''\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"markdown","source":"#### Параметры для LGB is_consumption=1","metadata":{}},{"cell_type":"markdown","source":"#### Параметры для LGB is_consumption=0","metadata":{}},{"cell_type":"markdown","source":"#### Параметры для LGB is_consumption=1","metadata":{}},{"cell_type":"code","source":"# Исходные модели\nif is_gpu:\n    p1={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1569, 'learning_rate': 0.0954350912359592, 'colsample_bytree': 0.5643173334694846, 'colsample_bynode': 0.8507963404594194, 'reg_alpha': 0.5815184353302838, 'reg_lambda': 19.577675580975, 'min_child_samples': 83, 'max_depth': 15, 'max_bin': 125}\n    \n    p2={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1655, 'learning_rate': 0.09749234861597421, 'colsample_bytree': 0.5878083872493073, 'colsample_bynode': 0.7476463591217907, 'reg_alpha': 0.6253329425224532, 'reg_lambda': 19.31104697178572, 'min_child_samples': 41, 'max_depth': 15, 'max_bin': 116}\n\n    p3={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1702, 'learning_rate': 0.08956547486313553, 'colsample_bytree': 0.553841254939378, 'colsample_bynode': 0.7707977076873439, 'reg_alpha': 3.4503195735482803, 'reg_lambda': 17.09137108374253, 'min_child_samples': 46, 'max_depth': 16, 'max_bin': 123}\n\n    p4={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1662, 'learning_rate': 0.09936096276241443, 'colsample_bytree': 0.5262782304338314, 'colsample_bynode': 0.6861424640373375, 'reg_alpha': 1.1434350179754031, 'reg_lambda': 18.238183112413253, 'min_child_samples': 42, 'max_depth': 16, 'max_bin': 118}\n\n    p5={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1596, 'learning_rate': 0.09145526299958297, 'colsample_bytree': 0.5813223295308532, 'colsample_bynode': 0.8019876742272201, 'reg_alpha': 1.7506271227424264, 'reg_lambda': 19.97101707600914, 'min_child_samples': 45, 'max_depth': 16, 'max_bin': 129}\n\n    p6={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1611, 'learning_rate': 0.09177637954991985, 'colsample_bytree': 0.5406014712979323, 'colsample_bynode': 0.7948709121454631, 'reg_alpha': 1.4162076884265264, 'reg_lambda': 19.18451476080634, 'min_child_samples': 39, 'max_depth': 15, 'max_bin': 128}\n\n    p7={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1722, 'learning_rate': 0.09721924587634734, 'colsample_bytree': 0.581822849292829, 'colsample_bynode': 0.8475502365142857, 'reg_alpha': 0.5558295705320715, 'reg_lambda': 17.676848729848402, 'min_child_samples': 76, 'max_depth': 15, 'max_bin': 121}\n    \n    p8={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 3080, 'verbose': -1, 'random_state': 49, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.0706777084688013, 'colsample_bytree': 0.8626666525958948, 'colsample_bynode': 0.764542591615078, 'reg_alpha': 17.891843863642414, 'reg_lambda': 13.11440305667691, 'min_child_samples': 13, 'max_depth': -1, 'max_bin': 195}\n    p9={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 3080, 'verbose': -1, 'random_state': 50, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.0706777084688013, 'colsample_bytree': 0.8626666525958948, 'colsample_bynode': 0.764542591615078, 'reg_alpha': 17.891843863642414, 'reg_lambda': 13.11440305667691, 'min_child_samples': 13, 'max_depth': -1, 'max_bin': 195}\n    p10={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 3080, 'verbose': -1, 'random_state': 51, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.0706777084688013, 'colsample_bytree': 0.8626666525958948, 'colsample_bynode': 0.764542591615078, 'reg_alpha': 17.891843863642414, 'reg_lambda': 13.11440305667691, 'min_child_samples': 13, 'max_depth': -1, 'max_bin': 195}\n    p11={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 3080, 'verbose': -1, 'random_state': 52, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.0706777084688013, 'colsample_bytree': 0.8626666525958948, 'colsample_bynode': 0.764542591615078, 'reg_alpha': 17.891843863642414, 'reg_lambda': 13.11440305667691, 'min_child_samples': 13, 'max_depth': -1, 'max_bin': 195}\n    p12={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 3080, 'verbose': -1, 'random_state': 53, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.0706777084688013, 'colsample_bytree': 0.8626666525958948, 'colsample_bynode': 0.764542591615078, 'reg_alpha': 17.891843863642414, 'reg_lambda': 13.11440305667691, 'min_child_samples': 13, 'max_depth': -1, 'max_bin': 195}\n    p13={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 3080, 'verbose': -1, 'random_state': 54, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.0706777084688013, 'colsample_bytree': 0.8626666525958948, 'colsample_bynode': 0.764542591615078, 'reg_alpha': 17.891843863642414, 'reg_lambda': 13.11440305667691, 'min_child_samples': 13, 'max_depth': -1, 'max_bin': 195}\n    p14={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2133, 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'num_leaves': 48, 'learning_rate': 0.07013300284120705, 'colsample_bytree': 0.9253282723262372, 'colsample_bynode': 0.8346343958070863, 'reg_alpha': 16.241601828339515, 'reg_lambda': 5.350458175997673, 'min_child_samples': 76, 'max_depth': -1, 'max_bin': 162}\nelse:\n    pass\n\n'''\np1={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2133, 'verbose': -1, 'objective': 'l2', 'num_leaves': 48, 'learning_rate': 0.07013300284120705, 'colsample_bytree': 0.9253282723262372, 'colsample_bynode': 0.8346343958070863, 'reg_alpha': 16.241601828339515, 'reg_lambda': 5.350458175997673, 'min_child_samples': 76, 'max_depth': -1, 'max_bin': 162}\np2={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2361, 'verbose': -1, 'objective': 'l2', 'num_leaves': 34, 'learning_rate': 0.060949366914939414, 'colsample_bytree': 0.9993288380159902, 'colsample_bynode': 0.9228215866566277, 'reg_alpha': 18.12946199946793, 'reg_lambda': 2.5957584406928143, 'min_child_samples': 85, 'max_depth': -1, 'max_bin': 163}\np3={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2157, 'verbose': -1, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.072162968496851, 'colsample_bytree': 0.9366582140649763, 'colsample_bynode': 0.9987768911165109, 'reg_alpha': 19.955878587229964, 'reg_lambda': 5.808092072452236, 'min_child_samples': 84, 'max_depth': -1, 'max_bin': 156}\np4={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2474, 'verbose': -1, 'objective': 'l2', 'num_leaves': 29, 'learning_rate': 0.06309418972836117, 'colsample_bytree': 0.84524749955274, 'colsample_bynode': 0.8613374079014886, 'reg_alpha': 16.597197944229823, 'reg_lambda': 3.226077180374126, 'min_child_samples': 57, 'max_depth': -1, 'max_bin': 133}\np5={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2279, 'verbose': -1, 'objective': 'l2', 'num_leaves': 26, 'learning_rate': 0.06613865527933853, 'colsample_bytree': 0.9742566223807109, 'colsample_bynode': 0.7969119918576429, 'reg_alpha': 18.7828897848879, 'reg_lambda': 4.246312305249892, 'min_child_samples': 20, 'max_depth': -1, 'max_bin': 156}\np6={'device': 'gpu', 'gpu_platform_id': 1, 'gpu_device_id': 0, 'n_estimators': 2315, 'verbose': -1, 'objective': 'l2', 'num_leaves': 25, 'learning_rate': 0.058945967929007116, 'colsample_bytree': 0.8404315490378437, 'colsample_bynode': 0.7845873376266698, 'reg_alpha': 18.72682046336603, 'reg_lambda': 3.92605604267166, 'min_child_samples': 128, 'max_depth': -1, 'max_bin': 108}\n'''\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\np1={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1569, 'learning_rate': 0.0954350912359592, 'colsample_bytree': 0.5643173334694846, 'colsample_bynode': 0.8507963404594194, 'reg_alpha': 0.5815184353302838, 'reg_lambda': 19.577675580975, 'min_child_samples': 83, 'max_depth': 15, 'max_bin': 125}\np2={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1569, 'learning_rate': 0.0954350912359592, 'colsample_bytree': 0.5643173334694846, 'colsample_bynode': 0.8507963404594194, 'reg_alpha': 0.5815184353302838, 'reg_lambda': 19.577675580975, 'min_child_samples': 83, 'max_depth': 15, 'max_bin': 125}    \np3={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1655, 'learning_rate': 0.09749234861597421, 'colsample_bytree': 0.5878083872493073, 'colsample_bynode': 0.7476463591217907, 'reg_alpha': 0.6253329425224532, 'reg_lambda': 19.31104697178572, 'min_child_samples': 41, 'max_depth': 15, 'max_bin': 116}\np4={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1655, 'learning_rate': 0.09749234861597421, 'colsample_bytree': 0.5878083872493073, 'colsample_bynode': 0.7476463591217907, 'reg_alpha': 0.6253329425224532, 'reg_lambda': 19.31104697178572, 'min_child_samples': 41, 'max_depth': 15, 'max_bin': 116}\np5={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1702, 'learning_rate': 0.08956547486313553, 'colsample_bytree': 0.553841254939378, 'colsample_bynode': 0.7707977076873439, 'reg_alpha': 3.4503195735482803, 'reg_lambda': 17.09137108374253, 'min_child_samples': 46, 'max_depth': 16, 'max_bin': 123}\np6={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1702, 'learning_rate': 0.08956547486313553, 'colsample_bytree': 0.553841254939378, 'colsample_bynode': 0.7707977076873439, 'reg_alpha': 3.4503195735482803, 'reg_lambda': 17.09137108374253, 'min_child_samples': 46, 'max_depth': 16, 'max_bin': 123}\np7={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1662, 'learning_rate': 0.09936096276241443, 'colsample_bytree': 0.5262782304338314, 'colsample_bynode': 0.6861424640373375, 'reg_alpha': 1.1434350179754031, 'reg_lambda': 18.238183112413253, 'min_child_samples': 42, 'max_depth': 16, 'max_bin': 118}\np8={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1662, 'learning_rate': 0.09936096276241443, 'colsample_bytree': 0.5262782304338314, 'colsample_bynode': 0.6861424640373375, 'reg_alpha': 1.1434350179754031, 'reg_lambda': 18.238183112413253, 'min_child_samples': 42, 'max_depth': 16, 'max_bin': 118}\np9={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1596, 'learning_rate': 0.09145526299958297, 'colsample_bytree': 0.5813223295308532, 'colsample_bynode': 0.8019876742272201, 'reg_alpha': 1.7506271227424264, 'reg_lambda': 19.97101707600914, 'min_child_samples': 45, 'max_depth': 16, 'max_bin': 129}\np10={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1596, 'learning_rate': 0.09145526299958297, 'colsample_bytree': 0.5813223295308532, 'colsample_bynode': 0.8019876742272201, 'reg_alpha': 1.7506271227424264, 'reg_lambda': 19.97101707600914, 'min_child_samples': 45, 'max_depth': 16, 'max_bin': 129}\np11={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1611, 'learning_rate': 0.09177637954991985, 'colsample_bytree': 0.5406014712979323, 'colsample_bynode': 0.7948709121454631, 'reg_alpha': 1.4162076884265264, 'reg_lambda': 19.18451476080634, 'min_child_samples': 39, 'max_depth': 15, 'max_bin': 128}\np12={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1611, 'learning_rate': 0.09177637954991985, 'colsample_bytree': 0.5406014712979323, 'colsample_bynode': 0.7948709121454631, 'reg_alpha': 1.4162076884265264, 'reg_lambda': 19.18451476080634, 'min_child_samples': 39, 'max_depth': 15, 'max_bin': 128}\n'''\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Параметры для LGB is_consumption=0","metadata":{}},{"cell_type":"code","source":"# Исходные модели\nif is_gpu:\n    # Параметры для lgbm c GPU\n    c1={'device': 'gpu', 'verbose': -1, 'random_state' : 42, 'objective': 'l2', 'n_estimators': 1961, 'learning_rate': 0.055041729559669385, 'colsample_bytree': 0.8054200071555299, 'colsample_bynode': 0.8827333010526346, 'reg_alpha': 9.649253442752036, 'reg_lambda': 16.98908601233005, 'min_child_samples': 54, 'max_depth': 12, 'max_bin': 36}\n\n    c2={'device': 'gpu', 'verbose': -1, 'random_state' : 43, 'objective': 'l2', 'n_estimators': 1948, 'learning_rate': 0.06233594141892915, 'colsample_bytree': 0.8484245099171761, 'colsample_bynode': 0.899824429438312, 'reg_alpha': 10.7294451589117, 'reg_lambda': 17.69396992827211, 'min_child_samples': 45, 'max_depth': 13, 'max_bin': 32}\n\n    c3={'device': 'gpu', 'verbose': -1, 'random_state' : 44, 'objective': 'l2', 'n_estimators': 1995, 'learning_rate': 0.0633853242094111, 'colsample_bytree': 0.9331894149297775, 'colsample_bynode': 0.972632605889707, 'reg_alpha': 8.983517796023829, 'reg_lambda': 18.03334867391121, 'min_child_samples': 33, 'max_depth': 11, 'max_bin': 48}\n\n    c4={'device': 'gpu', 'verbose': -1, 'random_state' : 45, 'objective': 'l2', 'n_estimators': 1948, 'learning_rate': 0.04980982203453618, 'colsample_bytree': 0.7850607568025258, 'colsample_bynode': 0.9990467893228645, 'reg_alpha': 8.279232611894738, 'reg_lambda': 18.878856721521696, 'min_child_samples': 85, 'max_depth': 12, 'max_bin': 32}\n\n    c5={'device': 'gpu', 'verbose': -1, 'random_state' : 46, 'objective': 'l2', 'n_estimators': 1958, 'learning_rate': 0.06331649649993518, 'colsample_bytree': 0.965107198312526, 'colsample_bynode': 0.9562601410444004, 'reg_alpha': 8.595100697458118, 'reg_lambda': 19.672841466470988, 'min_child_samples': 38, 'max_depth': 14, 'max_bin': 37}\n\n    c6={'device': 'gpu', 'verbose': -1, 'random_state' : 47, 'objective': 'l2', 'n_estimators': 1667, 'learning_rate': 0.06761829944908236, 'colsample_bytree': 0.8491878204722972, 'colsample_bynode': 0.7943301198687824, 'reg_alpha': 12.053887346204482, 'reg_lambda': 17.21503593146002, 'min_child_samples': 48, 'max_depth': 12, 'max_bin': 60}\n\n    c7={'device': 'gpu', 'verbose': -1, 'random_state' : 48, 'objective': 'l2', 'n_estimators': 1965, 'learning_rate': 0.06236463049661708, 'colsample_bytree': 0.9209979630548757, 'colsample_bynode': 0.9718755549743984, 'reg_alpha': 9.485421137815203, 'reg_lambda': 19.728001606564117, 'min_child_samples': 24, 'max_depth': 14, 'max_bin': 35}\n\n    c8={'device': 'gpu', 'n_estimators': 3637, 'verbose': -1, 'random_state': 49, 'objective': 'l2', 'num_leaves': 46, 'learning_rate': 0.03549176718139673, 'colsample_bytree': 0.9642490409419204, 'colsample_bynode': 0.312181596851313, 'reg_alpha': 3.2669304575194196, 'reg_lambda': 16.3440499988092, 'min_child_samples': 25, 'max_depth': -1, 'max_bin': 36}\nelse:\n    pass\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Функция выделения интервалов","metadata":{}},{"cell_type":"code","source":"# Выделяыет из данных указанные интервалы.\n# И в каждом интервале выбирает случайно лишь указанную долю данных\n# df_train - датафрейм для обработки\n# is_consumption какие данные возвращать данные в разрезе is_consumption\n# data_block_id_intervals какие данные возвращать данные в разрезе is_consumption\n#   это массив масивов. В каждой строке описание периода\n#   первая колонка на сколько data_block_id в конеце обучени отстоит от доступного конца данных\n#   вторая колонка сколько data_block_id будет в периоде на котором обучаемся.\n#   Третья колонка какую долю от данных оставдять (от 0 до 1)\n#   Периодов (строк) может быть произвольное количество\n# data_block_id_min минмимальный data_block_id который будет в возвращаемых данных\ndef get_train_intervals(df_train, is_consumption, data_block_id_intervals, data_block_id_min):\n    max_block_id = df_train[\"data_block_id\"].max()\n    \n    df_train_int = df_train[(\n        #  выбираем только те данные которые больше data_block_id_min\n        (df_train['data_block_id']>=data_block_id_min)\n        #  выбираем только те данные для обучения по is_consumption на которых специализируетсмя модель\n        &(df_train['is_consumption']==is_consumption)\n        # Оставляем только notnull таргеты\n        &(df_train[\"target\"].notnull())\n    )]\n\n    ind = 0\n    for cur_interval in data_block_id_intervals:\n        # вырезаем очередной дата блок в указанных границах\n        cur_data_block = df_train_int[(\n            # До какого data_block_id учим первый блок\n            (df_train_int['data_block_id']<=max_block_id-cur_interval[0])\n            # C какого data_block_id учим первый блок\n            &(df_train_int['data_block_id']>(max_block_id-cur_interval[0]-cur_interval[1]))\n        )]\n        # print('1. cur_data_block:', cur_data_block['data_block_id'].nunique())\n        # print('1.1 cur_data_block:', cur_data_block.shape[0])\n        # Берем только часть случайную часть данных из блока\n        cur_data_block = cur_data_block.sample(frac = cur_interval[2])\n        # print('2. cur_data_block:', cur_data_block['data_block_id'].nunique())\n        # print('2.1 cur_data_block:', cur_data_block.shape[0])\n        if ind == 0:\n            final_train_df = cur_data_block.copy()\n        else:\n            final_train_df = pd.concat([final_train_df, cur_data_block], ignore_index=True)\n        ind += 1\n    return(final_train_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Функция обучения моделей с диска (Быстрая)","metadata":{}},{"cell_type":"code","source":"# Обучает модели и данные, записаные на диск\n# Быстрая. Отличается тем, что два процесса запускаются и ждут комманд,\n# А не завершаются после выполнения обучения модели\n# и записывает обученные модели обратно на диск\n# models_names - список имен моделей\n# models_path - путь на диске где хроанятся модели и данные\n\ndef fit_models_from_disk_fast(models_names, models_path):\n    # Код процесса, который будет выполнятся параллельно\n    process_code = '''\nimport os\nimport time\nimport lightgbm as lgb\nimport pandas as pd\nfrom joblib import dump\nfrom joblib import load\n\nmodels_path = 'zzzmodels_pathzzz'\n\nwhile True:\n    model_name = input()\n    if model_name == 'exit':\n        print(f\"Finish\")\n        break\n    \n    # Загружаем модель\n    model = load(os.path.join(models_path, f'{model_name}.joblib'))\n    # Загружаем трейн\n    df_train = pd.read_pickle(os.path.join(models_path, f'{model_name}-df_train.pkl'))\n    sample_weight = df_train['sample_weight'].values\n    # Обучаем модель\n    model.fit(\n        X=df_train.drop(columns=[\"target\", \"data_block_id\", \"sample_weight\"]),\n        y=df_train[\"target\"],\n        sample_weight=sample_weight\n    )\n    \n    # Обучаем модель\n    model.fit(X, y)\n    \n    dump(model, os.path.join(models_path, f'{model_name}.joblib'))\n    \n    print(f\"Complete: {model_name}\")\n'''\n    process_code = process_code.replace(\"zzzmodels_pathzzz\", models_path)\n    process_code1 = process_code\n    process_code2 = process_code\n    # Запуск дочерних процесса в котором будут тренироваться модели\n    # Процесс 1\n    process1 = subprocess.Popen(['python', '-c', process_code1],\n                                stdin=subprocess.PIPE, \n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE,\n                                text=True)\n    # Процесс 2\n    process2 = subprocess.Popen(['python', '-c', process_code2],\n                                stdin=subprocess.PIPE, \n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE,\n                                text=True)\n    # Перебираем все модели, которые будем тренировать\n    # По две сразу. Если моделей нечетное model_name2 в последней итерации будет Nono\n    for model_name1, model_name2 in zip_longest(\n        models_names[::2], models_names[1::2], fillvalue=None):\n\n        print(p_time(), 'fit model:', model_name1)\n        #print(p_time(),'Start1.1')\n        # Отправка команды обучать модель дочернему процессу1\n        process1.stdin.write(f'{model_name1}\\n')\n        process1.stdin.flush()\n\n        if model_name2 is not None:\n            print(p_time(), 'fit model:', model_name2)\n            #print(p_time(),'Start1.2')\n            # Отправка команды обучать модель дочернему процессу2\n            process2.stdin.write(f'{model_name2}\\n')\n            process2.stdin.flush()\n        '''\n        # Проверка ошибки выполанния дочернего процесса\n        # **************************\n        process1.wait()\n        # Вывод результатов\n        output1, error1 = process1.communicate()\n        if output1:\n            print(\"Вывод программы 1:\")\n            print(output1)\n        if error1:\n            print(\"Ошибка выполнения программы 1:\")\n            print(error1)\n        # **************************\n        '''\n        #print(p_time(),'Start2')\n        # Ждем сообщения о завершенеии тренировки модели от дочерних процессов\n        response1 = process1.stdout.readline().strip()\n        if model_name2 is not None:\n            response2 = process2.stdout.readline().strip()\n        #print(p_time(),'Start3')\n        \n        print(p_time(),f\"Дочерний процесс1: {response1.strip()}\")\n        if model_name2 is not None:\n            print(p_time(),f\"Дочерний процесс2: {response2.strip()}\")\n        \n    # Отправка команды завершения\n    #response, _ = process.communicate(input='exit\\n')\n    process1.stdin.write('exit\\n')\n    process1.stdin.flush()\n    if model_name2 is not None:\n        process2.stdin.write('exit\\n')\n        process2.stdin.flush()\n    \n    # Ожидание завершения дочернего процесса\n    #print(p_time(),'Start4')\n    process1.wait()\n    if model_name2 is not None:\n        process2.wait()\n    #print(p_time(),'Start5')\n    \n    print(p_time(), \"Обучение моделей параллельно завершено\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Функция обучения-предсказания моделей c диска","metadata":{}},{"cell_type":"code","source":"# Возвращает строку с дочерним процессом для обучения и предсказания указанных моделей\n# models_names_to_train - список имен моделей, которые нужно тренировать\n# models_names_to_test - список имен моделей, которые нужно тестировать\n# models_path - путь на диске где хроанятся модели и данные\n# gpu_device_id - Номер графического процессора на котором будет выполнятся\n\ndef get_proc_code(models_names_to_train, models_names_to_test,\n                  models_path, drop_cols, is_consumption, gpu_device_id):\n    process_code = f'''\nimport os\nimport time\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom joblib import dump\nfrom joblib import load\n\nprocess_starttime = time.time()\n\n# Возвращает сколько уже работает процесс\ndef p_time():\n    run_time = round(time.time() - process_starttime)\n    return '    '+str(run_time).zfill(5)+' sec:'\n    \nmodels_path = '{models_path}'\nmodels_names_to_train = {models_names_to_train}\nmodels_names_to_test = {models_names_to_test}\ndrop_cols = {drop_cols}\nis_consumption = {is_consumption}\n\n# Загружаем данные для предсказания\nX_test = pd.read_pickle(os.path.join(models_path, f'x_test.pkl'))\n# Если берутся одинаково датаблоки дропов от первой модели для всех\n# df_train = pd.read_pickle(os.path.join(models_path, 'df_train.pkl'))\n\n# Перебираем модели, с помощью которых нужно сделать предсказания\n# в этом дочернем процесс\nfor model_name in models_names_to_test:\n    # Загружаем модель\n    model = load(os.path.join(models_path, model_name+'.joblib'))\n    model.set_params(device='{gpu_type}',\n                     n_jobs={n_jobs},\n                     gpu_platform_id=0,\n                     gpu_device_id={gpu_device_id})\n    model_drop_cols = drop_cols[model_name] + ['data_block_id']\n    model_cons = is_consumption[model_name]\n    if model_name in models_names_to_train:\n        # Если модель в списке для тренировки\n        # Загружаем трейн\n        print(p_time(), 'fit model:', model_name)\n        df_train = pd.read_pickle(os.path.join(models_path, model_name+'-df_train.pkl'))\n        # Обучаем модель\n        model.fit(\n            # Если берутся одинаково датаблоки дропов от первой модели для всех\n            # X=df_train.drop(columns=model_drop_cols+['target']),\n            X=df_train.drop(columns=[\"target\", \"data_block_id\"]),\n            y=df_train[\"target\"]\n            # Если берутся одинаково датаблоки дропов от первой модели для всех\n            # X=df_train[df_train['is_consumption']==model_cons].drop(columns=model_drop_cols+['target']),\n            # y=df_train[df_train['is_consumption']==model_cons][\"target\"]\n            # для предсказаний дифов\n            # y=df_train[\"target\"] - df_train[\"target_1\"].fillna(0)\n        )\n        dump(model, os.path.join(models_path, model_name+'.joblib'))\n    \n    # Делаем новый предикт\n    print(p_time(), 'predict model:', model_name)\n    # print('model_drop_cols:', model_drop_cols)\n    y_pred = model.predict(X_test.drop(columns=model_drop_cols)).clip(0)\\\n    # для предсказаний дифов\n    #y_pred = np.clip(\n    #    X_test[\"target_mean_1\"].fillna(0).values\n    #    + model.predict(X_test.drop(columns=model_drop_cols)),\n    #    0,\n    #    np.inf\n    #)\n    # Сохранение предсказаний в файл\n    np.save(os.path.join(models_path, model_name+'-y_pred'), y_pred, allow_pickle=True)\n    \nprint(p_time(), f\"Complete\")\n'''\n    return process_code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Обучает модели и данные, записаные на диск\n# Простая. Отличается тем, что процесс просто запускается,\n# сразу обучают модель и завершает работу\n# и записывает обученные модели обратно на диск\n# models_names_to_train - список имен моделей, которые нужно тренировать\n# models_names_to_test - список имен моделей, которые нужно тестировать\n# models_path - путь на диске где хроанятся модели и данные\n\ndef fit_predict_models_from_disk(\n    models_names_to_train, models_names_to_test, models_path, drop_cols, is_consumption):\n    print(p_time(), \"Обучение моделей параллельно\")\n    \n    # Сортируем models_names_to_test, чтобы сначала в нем шли модели из models_names_to_train\n    # Чтобы равномерно распределить модели для тренировки между процессами\n    models_names_to_test = sorted(\n        models_names_to_test,\n        key=lambda x: (\n            x not in models_names_to_train,\n            models_names_to_train.index(x)\n            if x in models_names_to_train\n            else float('inf')))\n    \n    # Разделение моделей для трейна на группы для каждого процесса\n    model_groups = [models_names_to_test[i::num_processes] for i in range(num_processes)]\n\n    processes = []\n\n    for process_id, model_group in enumerate(model_groups):\n        # Получаем код программы для процесса\n        proc_code = get_proc_code(\n            models_names_to_train=models_names_to_train,\n            models_names_to_test=model_group,\n            models_path=models_path,\n            drop_cols=drop_cols,\n            is_consumption=is_consumption,\n            gpu_device_id=(process_id % gpus_n))\n\n        # Записываем код в файл\n        proc_filename = os.path.join(models_path, f'process_{process_id}.py')\n        with open(proc_filename, 'w', encoding='utf-8') as file:\n            file.write(proc_code)\n            \n        process = subprocess.Popen(\n            ['python', proc_filename],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        processes.append(process)\n        # print('process_id:',process_id)\n        # print((process_id % gpus_n))\n\n    # Выставим process_terminated в True, если прерывали процесс\n    process_terminated = False\n    # Ожидание завершения всех процессов\n    for idx, process in enumerate(processes):\n        # Время сколько ждем максимально заверщения процесса\n        wait_time = max((stop_train_time - time.time()), 1)\n        if (is_disable_run_time_limit or (not models_names_to_train)):\n            # Если отключили таймаут или не обучаем модели просто ждем процесс\n            process.wait()\n        else:\n            try:\n                process.wait(timeout=wait_time)\n            except subprocess.TimeoutExpired:\n                # Если время ожидания превышено, принудительно завершаем процесс\n                process_terminated = True\n                process.terminate()\n\n        if not process_terminated:\n            output, error = process.communicate()\n            if output:\n                print(f\"Дочерний процесс {idx}:\")\n                print(output)\n            if is_local and error:\n                print(f\"Ошибка выполнения программы {idx}:\")\n                print(error)\n        else:\n            print(p_time(), f\"Прервали принудительно процесс {idx} обучение моделей\")\n\n    if process_terminated:\n        # Заново запускаем предсказания но уже без тренировки моделей\n        # если было превышено время для тренировки моделей\n        fit_predict_models_from_disk([], models_names_to_test, models_path, drop_cols, is_consumption)\n    \n    print(p_time(), \"Обучение моделей параллельно завершено\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Стрелочник. Выбирает какое обучение запустить быстрое или простое\n# models_names - список имен моделей\n# models_path - путь на диске где хроанятся модели и данные\ndef fit_models_from_disk(models_names, models_path):\n    #fit_models_from_disk_simple(models_names, models_path)\n    fit_models_from_disk_fast(models_names, models_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Класс моделей","metadata":{}},{"cell_type":"code","source":"#***\n\n# Класс обертка для моделей. Хранит различные пареметры для обучения моделей.\n# Например диапазоны данных на которых учить модель, как часто обучать заново и другие параметры\nclass Models:\n\n    # Путь где хранятся модели, тренировочные и тестовые выборки для них \n    models_path = 'models'\n    \n    \n    # Инициализирует параметры обучения\n    # init_model - готовый объект модели для обучения\n    # alphas_* - Массивы с коэффициентами, на которые домножают предсказания\n    # alphas_1 - массив для моделей с предсказанием is_consumption == 1\n    # alphas_0 - массив для моделей с предсказанием is_consumption == 0\n    def __init__(self, alphas_1, alphas_0):\n        # Инициализируем словари\n        # Ключами во всех словарях будет имя модели\n\n        # Имена моделей\n        self.model_names = []\n        \n        # Словарь с моделями\n        self.models =  dict()\n        \n        # Словарь с описанием периодов обучения модели\n        # пока это матрица. из двух столбцов и двух строк в каждой строке описание периода\n        # первая колонка на сколько data_block_id в конеце обучени отстоит от доступного конца данных\n        # вторая колонка сколько data_block_id будет в периоде на котором обучаемся.\n        # data_block_id могут быть эквивалентны дням, но могут и отличаться, если данные будут подавать блоками не равными дням\n        # либо если будут разрывы в данных. Но они точно будут эквиваленты циклам предсказания\n        self.data_block_id_intervals  = dict()\n\n        # Словарь с указанием по какой минимальный data_block_id отрезать данные.\n        # То есть меньше data_block_id_min не берем данные для обучения в любом случае, чтобы не было определено в data_block_id_intervals\n        self.data_block_id_min  = dict()\n        \n        # Если 1, то модель предназначена для предсказания потребления электричества\n        # Если 0, то модель предназначена для предсказания производства электричества\n        self.is_consumption = dict()\n        \n        # Раз во сколько иттераций обучат модель\n        self.learn_again_period = dict()\n        \n        # Смещение для начала обучения модели. Добавляется к номеру итерации сабмита.\n        # Скажем если смещение 6 номер итерации 1, а обучаемся раз в чем итераций. То обучение будет в первуже итерацию сабмита.\n        self.learn_again_offset = dict()\n\n        # Признаки которые нужно дропнуть у модели\n        self.drop_cols = dict()\n        \n        # Время в секундах сколько заняло последнее обучение модели\n        self.last_learn_time = dict()\n\n        # Обучена ли модель\n        self.is_trained = dict()\n\n        # Предсказания модели\n        self.predictions = dict()\n        \n        # Коэффициент на который умножать предсказания модели\n        self.predictions_alpha = dict()\n        \n        #Номер итерации. Обновляется при вызове метода fit\n        self.itter_n = 0\n        \n        # Массивы с коэффициентами, на которые домножают предсказания\n        # В нулевом индексе массива если устаревание 0 дней, в 1 если 1 день устарела и так далее\n        # self.alphas = [41/240, 39/240]\n        self.alphas_1 = alphas_1\n        self.alphas_0 = alphas_0\n        \n        # Устанавливаем в True если будем возвращать сохраненные ранее предикты\n        self.is_saved_predict = False\n\n        if not os.path.exists(self.models_path):\n            # Создание каталога, если его нет\n            os.makedirs(self.models_path)\n    \n    \n    # Добавляет еще одну модель\n    # model_name - название модели\n    # new_model - объект модели\n    def add_model(self, model_name, new_model, is_consumption, data_block_id_intervals,\n                  data_block_id_min, learn_again_period, learn_again_offset, drop_cols = []):\n        \n        self.model_names.append(model_name)\n        self.models[model_name] = new_model\n        self.is_consumption[model_name] = is_consumption\n        self.data_block_id_intervals[model_name] = data_block_id_intervals\n        self.data_block_id_min[model_name] = data_block_id_min\n        self.learn_again_period[model_name] = learn_again_period\n        self.learn_again_offset[model_name] = learn_again_offset\n        self.drop_cols[model_name] = drop_cols\n        self.last_learn_time[model_name] = 0\n        self.is_trained[model_name] = False\n        self.predictions[model_name] = []\n        self.predictions_alpha[model_name] = []\n\n        # Сохранаяем начальную модель на диске\n        dump(new_model, os.path.join(self.models_path, f'{model_name}.joblib'))\n        \n        \n    # Обучает модель\n    # model_name - название модели\n    # df_train - датафрейм который содержит данные для обучения и целевой признак\n    def fit_one_model(self, model_name, df_train):\n        #print(p_time(), 'fit model:', model_name)\n        '''\n        #print(p_time(), 'fit3')\n        sample_weight = df_train['sample_weight'].values\n        self.models[model_name].fit(\n            X=df_train.drop(columns=[\"target\", \"data_block_id\", \"sample_weight\"]),\n            y=df_train[\"target\"],\n            sample_weight=sample_weight\n        )\n        #print(p_time(), 'fit5')\n        '''\n        #df_train.to_pickle(os.path.join(self.models_path, f'{model_name}-df_train.pkl'))\n        pass\n    \n    \n    # записывает коэффициент альфа на который домножать предсказание\n    # model_name - название модели\n    def make_alpha(self, model_name):\n        if (((self.itter_n + self.learn_again_offset[model_name])\n             % self.learn_again_period[model_name]) == 0):\n            # Если модель только что училась ставим коэффициент первый, поменьше\n            self.predictions_alpha[model_name] = (self.alphas_0[0]\n                                                  if self.is_consumption[model_name] == 0\n                                                  else self.alphas_1[0])\n        else:\n            # Если модель училась в прошлой итерации\n            # ставим коэффициент второй, побольше\n            self.predictions_alpha[model_name] = (self.alphas_0[1]\n                                                  if self.is_consumption[model_name] == 0\n                                                  else self.alphas_1[1])\n                                                  \n    '''\n    # записывает коэффициент альфа на который домножать предсказание\n    # model_name - название модели\n    def make_alpha(self, model_name):\n        \n        self.predictions_alpha[model_name] = (1/2 if model_name == 'model-1' else 1/6)\n    '''        \n    \n    # Инициализирует следующую итерацию обучения и предсказания\n    # itter_n - номер иттерации в сабмите.\n    def init_iter(self, itter_n):\n        self.itter_n = itter_n\n        # В этот списко будем заносить модели, которые нужно тренировать\n        self.model_names_to_train = []        \n    \n    # Обучает все добавленные модели для которых наступило время их обучения\n    # df_train - датафрейм который содержит данные для обучения и целевой признак\n    # если itter_n равен <=0. Значит первоначальное обучение и обучаем всем модели\n    def fit(self, df_train):\n        if self.is_saved_predict:\n            # Не обучаем модели, если в режиме сохраненных предсказаний\n            return()\n            \n        max_block_id = df_train[\"data_block_id\"].max()\n     \n        # Перебираем все модели\n        for model_name in self.learn_again_period:\n\n            # Проверяем не нужно ли прервать запись выборок для обучения по таймауту\n            if ((time.time() > stop_train_time)\n                and not(is_disable_run_time_limit)):\n                # Нужно прерывать обучение моделей\n                print(p_time(), 'Прерываем обучение моделей из цикла записи тренировочных выборок')\n                self.model_names_to_train = []\n                break\n            # Либо модель еще не тренирована\n            if ((not self.is_trained[model_name])\n                # Либо учим если номер итераиции в сабмите плюс смещение делится на целое на период обучения\n                or (((self.itter_n + self.learn_again_offset[model_name])\n                     % self.learn_again_period[model_name]) == 0)):\n\n                df_train_int = df_train[(\n                    #  выбираем только те данные которые больше data_block_id_min\n                    (df_train['data_block_id']>=self.data_block_id_min[model_name])\n                    #  выбираем только те данные для обучения по is_consumption на которых специализируетсмя модель\n                    &(df_train['is_consumption']==self.is_consumption[model_name])\n                    # Оставляем только notnull таргеты\n                    &(df_train[\"target\"].notnull())\n                )]\n\n                ind = 0\n                for cur_interval in self.data_block_id_intervals[model_name]:\n                    # вырезаем очередной дата блок в указанных границах\n                    cur_data_block = df_train_int[(\n                        # До какого data_block_id учим первый блок\n                        (df_train_int['data_block_id']<=max_block_id-cur_interval[0])\n                        # C какого data_block_id учим первый блок\n                        &(df_train_int['data_block_id']>(max_block_id-cur_interval[0]-cur_interval[1]))\n                    )]\n                    # print('1. cur_data_block:', cur_data_block['data_block_id'].nunique())\n                    # print('1.1 cur_data_block:', cur_data_block.shape[0])\n                    # Берем только часть случайную часть данных из блока\n                    cur_data_block = cur_data_block.sample(frac = cur_interval[2])\n                    # print('2. cur_data_block:', cur_data_block['data_block_id'].nunique())\n                    # print('2.1 cur_data_block:', cur_data_block.shape[0])\n                    if ind == 0:\n                        final_train_df = cur_data_block.copy()\n                    else:\n                        final_train_df = pd.concat([final_train_df, cur_data_block], ignore_index=True)\n                    ind += 1\n\n                '''\n                print('df_train_int:', df_train_int['data_block_id'].nunique())\n                print('df_train_int:', df_train_int.shape[0])\n                print('df_train:', df_train['data_block_id'].nunique())\n                print('df_train:', df_train.shape[0])\n                print('final_train_df:', final_train_df['data_block_id'].nunique())\n                print('final_train_df:', final_train_df.shape[0])\n                '''\n\n                # print('zzz:', self.drop_cols[model_name])\n                # Удаляем признаки, которые были указаны к удалению для текущей модели\n                final_train_df = final_train_df.drop(columns=self.drop_cols[model_name])\n\n                # Сохраняем трейн для дальнейшего обучения параллельными процессами\n                final_train_df.to_pickle(os.path.join(self.models_path, f'{model_name}-df_train.pkl'))\n                \n                self.model_names_to_train.append(model_name)\n                # Отмечаем что модель тренирована (по крайней мере будет тренирована)\n                self.is_trained[model_name] = True\n                \n    \n    # Делает предсказание всеми добавленными моделями и сводит их в одно предсказание\n    def predict(self, X_test):\n        # Создаем датафрейм для предсказаний\n        # Первый столбец содержит информацию 'is_consumption'\n        # Для каждой модели отдельный столбец с предсказаниями\n        # В predict_df_0 будут предсказания для моделей с is_consumption == 0\n        self.predict_df_0 = pd.DataFrame(X_test['is_consumption'])\n        # В predict_df_1 будут предсказания для моделей с is_consumption == 1\n        self.predict_df_1 = pd.DataFrame(X_test['is_consumption'])\n        \n        # Сохраняем X_test для дальнейшего предсказания по нему параллельными процессами\n        X_test.to_pickle(os.path.join(self.models_path, f'x_test.pkl'))\n        \n        # Обучаем модели, записаные на диск и делаем предсказание\n        fit_predict_models_from_disk(\n            models_names_to_train = self.model_names_to_train,\n            models_names_to_test = list(self.models.keys()),\n            models_path = self.models_path,\n            drop_cols = self.drop_cols,\n            is_consumption = self.is_consumption\n        )\n        \n        # Перебираем все модели и записываем что предсказать на диск\n        for model_name in self.learn_again_period:\n            self.make_alpha(model_name)\n            \n        # Перебираем все модели и загружаем предсказания\n        for model_name in self.learn_again_period:\n            # Делаем предсказание в соответвующий датафрейм в столбец модели\n            y_pred = np.load(os.path.join(self.models_path, f'{model_name}-y_pred.npy'), allow_pickle=True)\n            self.predictions[model_name].append(y_pred)\n            \n            if self.is_consumption[model_name] == 0: \n                self.predict_df_0[model_name] = y_pred\n                # Домножаем предсказание на коэффициент\n                self.predict_df_0[model_name] = self.predict_df_0[model_name] * self.predictions_alpha[model_name]\n            else:\n                self.predict_df_1[model_name] = y_pred\n                self.predict_df_1[model_name] = self.predict_df_1[model_name] * self.predictions_alpha[model_name]\n\n        # Суммируем предсказания моделей раздельно по датафреймам\n        self.predict_df_0['target'] = self.predict_df_0.iloc[:, 1:].sum(axis=1) #mean(axis=1)\n        self.predict_df_1['target'] = self.predict_df_1.iloc[:, 1:].sum(axis=1) #mean(axis=1)\n        \n        #self.predict_df_0['target'] = self.predict_df_0.iloc[:, 1:].mean(axis=1)\n        #self.predict_df_1['target'] = self.predict_df_1.iloc[:, 1:].mean(axis=1)\n        \n        # Сведение в одно предсказание потребления и производства электричества у просьюмеров\n        predict_df = self.predict_df_1[['is_consumption', 'target']]\n\n        predict_df.loc[predict_df['is_consumption']==0, 'target'] = self.predict_df_0.loc[self.predict_df_0['is_consumption']==0, 'target']\n        \n        return predict_df['target'].values\n\n\n    # Записывает в модели пустые предсказания\n    def dummy_predict(self, y_pred):\n        for model_name in self.learn_again_period:\n            # Записываем пустые предсказания в модели\n            self.predictions[model_name].append(y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tresk = np.load(os.path.join('models', f'model-1-y_pred.npy'), allow_pickle=True)\n#tresk.shape[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Добавление моделей","metadata":{}},{"cell_type":"code","source":"# Для удаления созданных фич для удобстава создаем датафрей и в нем ищем колонки\n# Но для сокорости можно просто задать список (отрабатывает за 11 секунд один раз в самом начале)\nX, y = df_data.drop(\"target\"), df_data.select(\"target\")\nX = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\ndf_train = to_pandas(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Возвращает список к удалению всех погодных лагов\n# кроме тех что сдыинуты на указанный день\n# lag_day - день сдвига, который не нужно удалять из лагов\ndef get_one_weather_lag(lag_day):\n    wheather_lag_cols = []\n    for i in [1,2,3,4,7]:\n        wheather_lag_cols.extend(\n            [col for col in df_train.columns\n             if (col.endswith((f'_fd{i}', f'_fl{i}', f'_hd{i}', f'_hl{i}')))\n                 and not col.endswith((f'_fd{lag_day}', f'_fl{lag_day}', f'_hd{lag_day}', f'_hl{lag_day}'))])\n    \n    return wheather_lag_cols\n\n# Возвращает список к удалению всех погодных лагов\n# кроме тех что сдыинуты на указанный день\n# lag_day - день сдвига, который не нужно удалять из лагов\ndef get_one_hour_weather_lag(lag_hour):\n    wheather_lag_cols = []\n    for i in [1,2]:\n        wheather_lag_cols.extend(\n            [col for col in df_train.columns\n             if (col.endswith((f'_forecast_{i}h', f'_forecast_local_{i}h')))\n                 and not col.endswith((f'_forecast_{lag_hour}h', f'_forecast_local_{lag_hour}h'))])\n    \n    return wheather_lag_cols\n\n# Удаляет элементы из списка\n# my_list список из которого удаляем эдлементы\n# strings_to_remove список строк которые нужно удалить из my_list\ndef remove_list_elements(my_list, strings_to_remove):\n    for string in strings_to_remove:\n        if string in my_list:\n            my_list.remove(string)\n    return my_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncons1_drop_cols = ['target_ratio_y_1','target_mean_y_1','target_std_y_1',\n                   'target_y_1','target_y_2','target_y_3','target_y_4',\n                   'target_y_5','target_y_6','target_y_7','target_y_8',\n                   'target_std_2','target_mean_2','target_ratio_2',\n                   #'direct_solar_radiation','diffuse_radiation'\n                  ]\n# Для cons1 удаляем все погодные лаги кроме как на неделю как было и раньше\n\ncons1_drop_cols.extend(get_one_hour_weather_lag(0))\ncons1_drop_cols.extend(get_one_weather_lag(7))\ncons1_drop_cols = remove_list_elements(\n    cons1_drop_cols,\n    ['hours_ahead_fd7',\n     'temperature_fd7',\n     'hours_ahead_fl7',\n     'temperature_fl7',\n     'temperature_hd7',\n     'temperature_hl7',\n     #'rad_sum',\n     #'rad_sum_hl',\n    ]\n)\ncons0_drop_cols = []\ncons0_drop_cols.extend(get_one_weather_lag(0))\n\nblock = [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]]\n#block_cons1 = [[0,30,1],[30,70,0.7],[100,100,0.25],[200,180,0.15],[380,1000,0.1]]\nblock_cons1 = [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]]\n# block = [[0,10000,1]]\n\n\nmodels_list = [\n    {'model_name': 'model-1', 'new_model': lgb.LGBMRegressor(**p1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    {'model_name': 'model-2', 'new_model': lgb.LGBMRegressor(**p2), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    {'model_name': 'model-3', 'new_model': lgb.LGBMRegressor(**p3), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    {'model_name': 'model-4', 'new_model': lgb.LGBMRegressor(**p4), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    {'model_name': 'model-5', 'new_model': lgb.LGBMRegressor(**p5), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    {'model_name': 'model-6', 'new_model': lgb.LGBMRegressor(**p6), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    #{'model_name': 'model-7', 'new_model': lgb.LGBMRegressor(**p7), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    #{'model_name': 'model-8', 'new_model': lgb.LGBMRegressor(**p8), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    #{'model_name': 'model-9', 'new_model': lgb.LGBMRegressor(**p9), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    #{'model_name': 'model-10', 'new_model': lgb.LGBMRegressor(**p10), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    #{'model_name': 'model-11', 'new_model': lgb.LGBMRegressor(**p11), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    #{'model_name': 'model-12', 'new_model': lgb.LGBMRegressor(**p12), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': block_cons1, 'data_block_id_min': 0, 'drop_cols': cons1_drop_cols},\n    {'model_name': 'model-solar-1', 'new_model': lgb.LGBMRegressor(**c1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    {'model_name': 'model-solar-4', 'new_model': lgb.LGBMRegressor(**c2), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    {'model_name': 'model-solar-2', 'new_model': lgb.LGBMRegressor(**c3), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    {'model_name': 'model-solar-3', 'new_model': lgb.LGBMRegressor(**c4), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    {'model_name': 'model-solar-5', 'new_model': lgb.LGBMRegressor(**c5), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    {'model_name': 'model-solar-6', 'new_model': lgb.LGBMRegressor(**c6), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-7', 'new_model': lgb.LGBMRegressor(**c7), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-8', 'new_model': lgb.LGBMRegressor(**c8), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-9', 'new_model': lgb.LGBMRegressor(**c9), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-10', 'new_model': lgb.LGBMRegressor(**c10), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-11', 'new_model': lgb.LGBMRegressor(**c11), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-12', 'new_model': lgb.LGBMRegressor(**c12), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-7', 'new_model': lgb.LGBMRegressor(**c7), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-8', 'new_model': lgb.LGBMRegressor(**c8), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-9', 'new_model': lgb.LGBMRegressor(**c9), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-10', 'new_model': lgb.LGBMRegressor(**c10), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-11', 'new_model': lgb.LGBMRegressor(**c11), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n    #{'model_name': 'model-solar-12', 'new_model': lgb.LGBMRegressor(**c12), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': block, 'data_block_id_min': 0, 'drop_cols': cons0_drop_cols},\n]\n\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Вариант 5","metadata":{}},{"cell_type":"code","source":"is_cons_1_n = sum(model['is_consumption'] == 1 for model in models_list)\nis_cons_0_n = sum(model['is_consumption'] == 0 for model in models_list)\n\n#print(\"Number of rows with is_consumption == 1:\", is_cons_1_n)\n#print(\"Number of rows with is_consumption == 0:\", is_cons_0_n)\n\n# Вариант с одинаковыми коэффцициентами и подстройкой под число моделей в списке\n# А вообще это коэффиценты для предсказания моделей обученный в этой итерации и в прошлой\nmodels = Models(alphas_1=[1/is_cons_1_n, 1/is_cons_1_n], alphas_0=[1/is_cons_0_n, 1/is_cons_0_n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Добавляем модели в объект класса моделей\nfor model_param in models_list:\n    models.add_model(**model_param)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Обучение моделей","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#if is_local:\n#    dump(model_solar, 'model_solar.joblib')\n#    dump(model, 'model_lgbm.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Расскоментировать при необходимости загрузку ранее сохраненных моделей","metadata":{}},{"cell_type":"code","source":"# Для загрузки локально\n#model_solar = load('model_solar.joblib')\n#model = load('model_lgbm.joblib')\n\n# Для загрузки на kaggle\n#model_solar = load('/kaggle/input/enefit/model_solar.joblib')\n#model = load('/kaggle/input/enefit/model_lgbm.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"if is_local:\n    # Если выполняем локально, а не сабмитим на кагл,\n    # то выбираем другое имя для файла submission.csv.\n    # Потому что в submission.csv записать прав нет и вылетает по ошибке\n    submission_name = 'submission_loc.csv'\nelse:\n    submission_name = 'submission.csv'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Содержимое public_timeseries_testing_util.py\n\nС необходимыми праками. Решил не импортировать его. а прямо тут. Так удобнее переносить на kaggle","metadata":{}},{"cell_type":"code","source":"'''\nAn unlocked version of the timeseries API intended for testing alternate inputs.\nMirrors the production timeseries API in the crucial respects, but won't be as fast.\n\nONLY works afer the first three variables in MockAPI.__init__ are populated.\n'''\n\nfrom typing import Sequence, Tuple\n\n\nclass MockApi:\n    def __init__(self):\n        '''\n        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n        They've been intentionally left in an invalid state.\n\n        Variables to set:\n            input_paths: a list of two or more paths to the csv files to be served\n            group_id_column: the column that identifies which groups of rows the API should serve.\n                A call to iter_test serves all rows of all dataframes with the current group ID value.\n            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n        '''\n        self.input_paths: Sequence[str] = [f'{test_path}/test.csv',\n                                   f'{test_path}/revealed_targets.csv', \n                                   f'{test_path}/client.csv',\n                                   f'{test_path}/historical_weather.csv',\n                                   f'{test_path}/forecast_weather.csv',\n                                   f'{test_path}/electricity_prices.csv',\n                                   f'{test_path}/gas_prices.csv',\n                                   f'{test_path}/sample_submission.csv']\n        self.group_id_column: str = 'data_block_id'\n        self.export_group_id_column: bool = False\n        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n        assert len(self.input_paths) >= 2\n\n        self._status = 'initialized'\n        self.predictions = []\n\n    def iter_test(self) -> Tuple[pd.DataFrame]:\n        '''\n        Loads all of the dataframes specified in self.input_paths,\n        then yields all rows in those dataframes that equal the current self.group_id_column value.\n        '''\n        if self._status != 'initialized':\n\n            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n\n        dataframes = []\n        for pth in self.input_paths:\n            dataframes.append(pd.read_csv(pth, low_memory=False))\n        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n\n        for group_id in group_order:\n            self._status = 'prediction_needed'\n            current_data = []\n            for df in dataframes:\n                cur_df = df.loc[group_id].copy()\n                # returning single line dataframes from df.loc requires special handling\n                if not isinstance(cur_df, pd.DataFrame):\n                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n                    cur_df.index.name = self.group_id_column\n                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n                current_data.append(cur_df)\n            yield tuple(current_data)\n\n            while self._status != 'prediction_received':\n                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n                yield None\n\n        with open(submission_name, 'w') as f_open:\n            pd.concat(self.predictions).to_csv(f_open, index=False)\n        self._status = 'finished'\n\n    def predict(self, user_predictions: pd.DataFrame):\n        '''\n        Accepts and stores the user's predictions and unlocks iter_test once that is done\n        '''\n        if self._status == 'finished':\n            raise Exception('You have already made predictions for the full test set.')\n        if self._status != 'prediction_needed':\n            raise Exception('You must get the next test sample from `iter_test()` first.')\n        if not isinstance(user_predictions, pd.DataFrame):\n            raise Exception('You must provide a DataFrame.')\n\n        self.predictions.append(user_predictions)\n        self._status = 'prediction_received'\n\n\ndef make_env():\n    return MockApi()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Функция для скора","metadata":{}},{"cell_type":"code","source":"# Вычисляет скор для предсказаний, который были поданы на вход\n# compare - датафрейм с уже заполенными реальными значениями таргета\n# и сделанными предсказаниями\n\ndef compute_compare(compare, index_name):\n    mae = mean_absolute_error(compare['target'] , compare['predict'])\n\n    if (compare['data_block_id'].max() >= 518):\n        compare_temp = compare[(compare['data_block_id'] >= 518)&(compare['data_block_id'] <= 606)]\n        mae_518_606 = mean_absolute_error(compare_temp['target'], compare_temp['predict'])\n    else:\n        # Если еще не дошли до data_block_id >= 518 не считаем эти величины\n        mae_518_606 = '-'\n    \n    if (compare['data_block_id'].max() > 600):\n        # Считаем MAE для data_block_id > 600\n        compare_temp = compare[compare['data_block_id'] > 600]\n        mae_600 = mean_absolute_error(compare_temp['target'], compare_temp['predict'])\n    else:\n        # Если еще не дошли до data_block_id > 600 не считаем эти величины\n        mae_600 = '-'\n\n    mae_df = pd.DataFrame({\n        '(ALL)': mae,\n        'Feb - Apr (518 - 606)': mae_518_606,\n        '(> 600)': mae_600\n    }, index=[index_name])\n\n    # Округляем числа до двух знаков после запятой и преобразуем их в строки\n    mae_df = mae_df.round(3).astype(str)\n    \n    return mae_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Подсчитывает скор.\n# Возвращает датафрейм compare со сравнением предсказаний\ndef calc_score():\n    # Загружаем предсказания\n    #submission = pd.read_csv(submission_name)\n    submission = pd.concat(env.predictions)\n    \n    # Загружаем истинные значения\n    revealed_targets = pd.read_csv(os.path.join(test_path, \"revealed_targets.csv\"))\n    revealed_targets['data_block_id'] -= 2\n    revealed_targets = revealed_targets[revealed_targets[\"data_block_id\"] > train_end_data_block_id]\n    # Обрезаем реальные предсказания revealed_targets по длине уже сделанных предсказаний submission\n    revealed_targets = revealed_targets.iloc[:len(submission)]\n\n    # print(f'MAE: {mae}')\n    \n    # Подготовим данные для анализа изменения ошибки предсказания по мере удаления от времени завершения обучения\n    compare = revealed_targets[['data_block_id', 'is_consumption', 'target']].copy()\n    compare['predict'] = submission['target'].values\n    compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n    compare['err'] = (compare['predict'] - compare['target']).values\n\n    mae_df = compute_compare(compare, 'All Models MAE')\n\n    new_mae_df = compute_compare(compare[compare['is_consumption']==0], 'is_consumption == 0')\n    mae_df = pd.concat([mae_df, new_mae_df])\n\n    new_mae_df = compute_compare(compare[compare['is_consumption']==1], 'is_consumption == 1')\n    mae_df = pd.concat([mae_df, new_mae_df])\n\n    # Перебираем предсказания всех моделей и складываем\n    # скоры их предсказания в один датафрейм mae_df\n    for model_name in models.predictions:\n        # заполняем compareпредсказаниями модели\n        compare['predict'] = np.concatenate(models.predictions[model_name])\n        compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n        compare['err'] = (compare['predict'] - compare['target']).values\n        # Оставляем в предсказаниях только те строки, которые предсказывала модель\n        new_compare = compare[compare['is_consumption']==models.is_consumption[model_name]] \n        \n        new_mae_df = compute_compare(new_compare,\n                                     model_name+f' ({models.is_consumption[model_name]})')\n                                                 \n        mae_df = pd.concat([mae_df, new_mae_df])\n    \n    display(mae_df)\n    print('MAE > 600', mae_df.loc['All Models MAE','(> 600)'])\n\n    compare['predict'] = submission['target'].values\n    compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n    compare['err'] = (compare['predict'] - compare['target']).values\n    \n    return compare","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Инициализация иттераций сабмита","metadata":{}},{"cell_type":"code","source":"if is_local:\n    # После этого можно имитировать локально загрузку при собмите на большом числе итераций\n    # А не только четыре иттерации на 4 дня как в стандартной имитайии на кагле\n    env = make_env()\nelse:\n    # загружаем оригинальную библиотеку для сабмита\n    import enefit\n    env = enefit.make_env()\n\niter_test = env.iter_test()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Цикл сабмита","metadata":{}},{"cell_type":"code","source":"# Находим последний data_block_id в обучающих данных\nmax_train_data_block_id = df_data[\"data_block_id\"].max()\n# Устанавливаем первый data_block_id для теста следущим за тренировочным\ncur_test_data_block_id = max_train_data_block_id + 1\ncur_test_data_block_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncount = 0\n\n# Основной цикл для обработки данных тестового набора\nfor (test, revealed_targets, client, historical_weather,\n        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n    iteration_start_time = time.time()\n    print(p_time(), f'*************** Iteration: {count}, data_block_id: {cur_test_data_block_id-1} ***************')\n    # iteration__scrored - устанавливается в True, если эту итерацию нужно предсказывать\n    iteration__scrored = (not (test['currently_scored'] == False).all())\n    \n    # Переименование столбца для удобства\n    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        \n    if is_local:\n        # Если выполняем локально, то преобразуем некоторые типы данных\n        # На кагле (а может и в линуксе) они и так преобразуются, но на виновс локально\n        # не преобразуются и выдетают по ощибке\n        test['datetime'] = pd.to_datetime(test['datetime'])\n        client['date'] = pd.to_datetime(client['date'])\n        gas_prices['origin_date'] = pd.to_datetime(gas_prices['origin_date'])\n        gas_prices['forecast_date'] = pd.to_datetime(gas_prices['forecast_date'])\n        electricity_prices['origin_date'] = pd.to_datetime(electricity_prices['origin_date'])\n        electricity_prices['forecast_date'] = pd.to_datetime(electricity_prices['forecast_date'])\n        forecast_weather['origin_datetime'] = pd.to_datetime(forecast_weather['origin_datetime'])\n        forecast_weather['forecast_datetime'] = pd.to_datetime(forecast_weather['forecast_datetime'])\n        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n        revealed_targets['datetime'] = pd.to_datetime(revealed_targets['datetime'])\n        \n    # Добавляем колонку заполненную следующим data_block_id\n    test[\"data_block_id\"] = cur_test_data_block_id\n    revealed_targets[\"data_block_id\"] = cur_test_data_block_id\n    \n    df_test            = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n    df_new_client      = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n    df_new_gas         = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n    df_new_electricity = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n    df_new_forecast    = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n    df_new_historical  = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n    df_new_target      = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n    df_new_data        = pl.from_pandas(revealed_targets[df_data_cols], schema_overrides=schema_data)\n    # Объединение новых данных с существующими и удаление дубликатов\n    df_client          = pl.concat([df_client, df_new_client]).unique(subset=[\"county\", \"is_business\", \"product_type\", \"date\"], maintain_order=True)\n    df_gas             = pl.concat([df_gas, df_new_gas]).unique(subset=[\"forecast_date\"], maintain_order=True)\n    df_electricity     = pl.concat([df_electricity, df_new_electricity]).unique(subset=[\"forecast_date\"], maintain_order=True)\n    df_forecast        = pl.concat([df_forecast, df_new_forecast]).unique()\n    df_historical      = pl.concat([df_historical, df_new_historical]).unique()\n    df_target          = pl.concat([df_target, df_new_target]).unique()\n    df_data            = pl.concat([df_data, df_new_data]).unique()\n        \n    if iteration__scrored:\n    # if True:\n        # Инициализируем итерацию обучения и предсказания\n        models.init_iter(itter_n=count)\n        cur_time = time.time()\n        if ((cur_time < stop_train_time) or is_disable_run_time_limit):\n            # Не начинаем тренировать модели заново на реальном сабмите пока в датах предсказания не появятся даты идущие в скор\n            X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n            X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n            #Добавились новые строки в данные. Учим.\n            df_train = to_pandas(X, y)\n            models.fit(df_train=df_train)\n        else:\n            print('Не тренеруем модели, превышено время выполнения ноутбука:', (cur_time - notebook_starttime))\n    \n        # Применение функции инженерии признаков и преобразование данных обратно в pandas\n        X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n        X_test = to_pandas(X_test)\n        \n        # Прогнозирование с использованием модели и ограничение предсказаний нулем\n        test['target'] = models.predict(X_test)\n        \n        # Обновление целевых значений в примере предсказания\n        sample_prediction[\"target\"] = test['target']\n        \n        # Отправка предсказаний в среду выполнения\n        env.predict(sample_prediction)\n    \n        if is_local:\n            # Выводим текущий скор в разных разрезах\n            compare = calc_score()\n            pass\n    else:\n        print(p_time(), 'Итерация не входит в скор, просто возвращаем sample_prediction')\n        env.predict(sample_prediction)\n        models.dummy_predict(sample_prediction[\"target\"].values)\n        \n    count += 1\n    # Переходим к следующему data_block_id на итерации тестов\n    cur_test_data_block_id += 1\n    print(p_time(), 'Iteration run time:', round(time.time() - iteration_start_time))\n    print('')\n    print('________________________________________________')\n    print('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Анализ предсказания","metadata":{}},{"cell_type":"markdown","source":"### Подсчет скора","metadata":{}},{"cell_type":"code","source":"if is_local:\n    compare = calc_score()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    dump(models.models['model-1'], 'models.joblib')\n    pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#models = load('models.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ncount = 0\nfor cur_predict in models.predictions['model-1']:\n    cur_predict = cur_predict * 0\n    print(cur_predict)\n    for model_name in models.predictions:\n        #print(models.predictions[model_name][count])\n        \n        break\n    count += 1\n    \n    break\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### График MAE по дням предсказания","metadata":{}},{"cell_type":"code","source":"# выводит график средних ошибок сгруппированных по дням (точнее для блоков данных для предсказаний которые в целом эквиваленты дням)\ndef print_err(err_name, err_lable, err_title):\n    # Группируем по data_block_id, то есть по дням и считаем отдельно для каждого дня предсказания MAE\n    grouped_compare = compare.groupby('data_block_id').mean().reset_index()\n    # Делаем скользящую среднюю\n    grouped_compare['rolling_mean'] = grouped_compare[err_name].rolling(window=30, min_periods=1).mean()\n    \n    \n    # Plotting the mean absolute errors\n    plt.figure(figsize=(10, 8))\n    #plt.bar(grouped_compare['data_block_id'], grouped_compare['abs_err'])\n    plt.bar(grouped_compare['data_block_id'], grouped_compare[err_name], label=err_lable)\n    plt.plot(grouped_compare['data_block_id'],\n             grouped_compare['rolling_mean'],\n             label='Rolling Mean (window=30)',\n             color='orange',\n             linestyle='-', linewidth=2)\n    plt.xlabel('data_block_id')\n    plt.ylabel(err_lable)\n    plt.title(err_title)\n    plt.legend()\n    \n    # Set ticks every 10 data_block_id\n    tick_positions_y = np.arange(-40, max(grouped_compare[err_name]) + 1, 10)\n    plt.yticks(tick_positions_y)\n    plt.grid(True)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    print_err(err_name='abs_err',\n              err_lable='Mean Absolute Error',\n              err_title='Mean Absolute Error by data_block_id')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    compare = compare[compare[\"data_block_id\"] > 600]\n    print_err(err_name='err',\n              err_lable='Mean Error (predict - target)',\n              err_title='Mean Error by data_block_id')","metadata":{},"execution_count":null,"outputs":[]}]}